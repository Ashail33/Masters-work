{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb3ZNfFDNX84NHofKHb9C/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashail33/Masters-work/blob/master/One%20script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment prep**"
      ],
      "metadata": {
        "id": "ImzJDJofLgxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up for data generation\n",
        "# !pip uninstall -y mdcgenpy\n",
        "!pip install git+https://github.com/Ashail33/mdcgenpy.git\n",
        "\n",
        "from  mdcgenpy import clusters as cl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "# import cl\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "tIGNr_dhLhW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Clustering Dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-BuB0D1XLSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The properties I will be adding to the dataset are:\n",
        "#     1) Outliers - Binary ( two options) - outliers\n",
        "#     2) Noise - Binary ( two options) add_noise=0,n_noise=None,\n",
        "#     3) Number of clusters - I will select three values k\n",
        "#     4) Number of data points - I will select three values ( 100 000, 1 000 000, 100 000 000)n_samples\n",
        "#     5) Number of features - I will select five values ( 2, 10, 50, 100, 500) n_feats\n",
        "#     6) Density - I will select three values compactness_factor\n",
        "#     --7) Cluster shape - I will select three types - clusters within a radius ,  hollow shaped clusters , s-shaped / c-shaped clusters\n",
        "#      distributions\n",
        "#         'uniform': lambda shape, param: np.random.uniform(-param, param, shape),\n",
        "#     'gaussian': lambda shape, param: np.random.normal(0, param, shape),\n",
        "#     'logistic': lambda shape, param: np.random.logistic(0, param, shape),\n",
        "#     'triangular': lambda shape, param: np.random.triangular(-param, 0, param, shape),\n",
        "#     'gamma': lambda shape, param: np.random.gamma(2 + 8 * np.random.rand(), param / 5, shape),\n",
        "#     'gap': lambda shape, param: gap(shape, param)\n",
        "\n",
        "# --8) Missing values - this will need to be created by randomly removing values up to a certain number of columns and records\n",
        "\n",
        "#Mount drive where you would like to add the data to\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Create the 'Masters_data' folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Parameters for generating datasets\n",
        "outliers = [0,500]\n",
        "noise_options = [(0, None), (10, 100)]\n",
        "n_clusters = [3, 5, 10]\n",
        "n_samples = [10000,50000, 100000]\n",
        "n_feats = [2, 10,100]\n",
        "compactness_factors = [0.01, 0.5, 1]\n",
        "\n",
        "# Distributions for cluster shapes\n",
        "dist_options = ['gaussian']\n",
        "\n",
        "# Find the last generated dataset\n",
        "existing_files = glob.glob(os.path.join(folder_path, 'dataset_*.csv'))\n",
        "dataset_numbers = [int(re.search(r'dataset_(\\d+).csv', file).group(1)) for file in existing_files]\n",
        "if dataset_numbers:\n",
        "    last_dataset_id = max(dataset_numbers)\n",
        "else:\n",
        "    last_dataset_id = -1\n",
        "\n",
        "metadata = []\n",
        "\n",
        "# Continue from the last generated dataset\n",
        "for idx, outlier in enumerate(outliers):\n",
        "    for idy, (add_noise, n_noise) in enumerate(noise_options):\n",
        "        for idz, k in enumerate(n_clusters):\n",
        "            for idw, n_sample in enumerate(n_samples):\n",
        "                for idv, n_feat in enumerate(n_feats):\n",
        "                    for idu, compactness_factor in enumerate(compactness_factors):\n",
        "                        for idt, distribution in enumerate(dist_options):\n",
        "                            current_dataset_id = (idx * len(noise_options) * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idy * len(n_clusters) * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idz * len(n_samples) * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idw * len(n_feats) * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idv * len(compactness_factors) * len(dist_options) +\n",
        "                                                  idu * len(dist_options) +\n",
        "                                                  idt)\n",
        "\n",
        "                            if current_dataset_id <= last_dataset_id:\n",
        "                                continue\n",
        "\n",
        "                            file_name = f'dataset_{current_dataset_id}.csv'\n",
        "                            file_path = os.path.join(folder_path, file_name)\n",
        "                            distributions = [distribution] * k\n",
        "\n",
        "\n",
        "                            cluster_gen = cl.ClusterGenerator(\n",
        "                                          n_samples=n_sample,\n",
        "                                          outliers=outlier,\n",
        "                                          n_feats=n_feat,\n",
        "                                          k=k,\n",
        "                                          distributions=distributions,\n",
        "                                          compactness_factor=compactness_factor,\n",
        "\n",
        "                                          )\n",
        "\n",
        "                            data = cluster_gen.generate_data(output_file=file_path)\n",
        "\n",
        "                            dataset_properties = {\n",
        "                                'id': current_dataset_id,\n",
        "                                'outliers': outlier,\n",
        "                                'add_noise': add_noise,\n",
        "                                'n_noise': n_noise,'n_clusters': k,\n",
        "                                'n_samples': n_sample,\n",
        "                                'n_feats': n_feat,\n",
        "                                'compactness_factor': compactness_factor,\n",
        "                                'distribution': distribution,\n",
        "                                'file_path': file_path\n",
        "                                }\n",
        "\n",
        "                            metadata.append(dataset_properties)\n",
        "                            metadata_file_path = os.path.join(folder_path, 'metadata.json')\n",
        "                            with open(metadata_file_path, 'w') as f:\n",
        "                                json.dump(metadata, f)\n"
      ],
      "metadata": {
        "id": "dvz7jsKwLe3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}