{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJDOjW0KphJDhU2DmXcFy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashail33/Masters-work/blob/master/Call_all_functions_and_run_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2LB8VkjJ-JU"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import cdist\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import tracemalloc\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import json\n",
        "\n",
        "#evaluation code\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.monotonic()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.interval = time.monotonic() - self.start_time\n",
        "\n",
        "\n",
        "class MemoryMonitor:\n",
        "    def __enter__(self):\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.memory_info = self.process.memory_info()\n",
        "\n",
        "\n",
        "class DiskMonitor:\n",
        "    def __enter__(self):\n",
        "        self.disk_read_start = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_start = psutil.disk_io_counters().write_bytes\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.disk_read_end = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_end = psutil.disk_io_counters().write_bytes\n",
        "        self.disk_read_bytes = self.disk_read_end - self.disk_read_start\n",
        "        self.disk_write_bytes = self.disk_write_end - self.disk_write_start\n",
        "\n",
        "\n",
        "def compactness(X, labels):\n",
        "    centroids = np.array([np.mean(X[np.where(labels == i)], axis=0) for i in set(labels.flatten())])\n",
        "    distances = cdist(X, centroids)\n",
        "    return np.sum(np.min(distances, axis=1))\n",
        "\n",
        "\n",
        "def separation(X, labels):\n",
        "    centroids = np.array([np.mean(X[np.where(labels == i)], axis=0) for i in set(labels.flatten())])\n",
        "    return np.sum(cdist(centroids, centroids))\n",
        "\n",
        "\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    k = len(set(labels.flatten()))\n",
        "    clusters = [X[np.where(labels == i)] for i in range(k)]\n",
        "    cluster_distances = np.array([np.max(cdist(c1, c2)) for i, c1 in enumerate(clusters) for j, c2 in enumerate(clusters) if i < j])\n",
        "    \n",
        "    pairwise_distances = cdist(X, X)\n",
        "    min_inter_cluster_distances = np.min(pairwise_distances[np.where(labels[:, None] != labels[None, :])])\n",
        "    \n",
        "    return min_inter_cluster_distances / np.max(cluster_distances)\n",
        "\n",
        "# code to evaluate the clustering algorithms\n",
        "def evaluate_clustering(algorithm, dataset, true_labels=None, n_runs=10):\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    metric_function_map = {\n",
        "        'cp': compactness,\n",
        "        'sp': separation,\n",
        "        'db': davies_bouldin_score,\n",
        "        'silhouette': silhouette_score,\n",
        "        'calinski_harabasz': calinski_harabasz_score,\n",
        "        'ari': adjusted_rand_score,\n",
        "        'nmi': normalized_mutual_info_score,\n",
        "        'dvi': dunn_index,\n",
        "    }\n",
        "\n",
        "    previous_runs_labels = []\n",
        "    previous_runs_results = []\n",
        "\n",
        "    # Start memory monitoring before the loop\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        with Timer() as t, DiskMonitor() as d:\n",
        "            labels = algorithm.fit_predict(dataset)\n",
        "\n",
        "        # Memory usage tracking\n",
        "        current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "        results['memory_usage'].append(current_memory)\n",
        "        results['memory_peak'].append(peak_memory)\n",
        "\n",
        "        results['runtime'].append(t.interval)\n",
        "        results['disk_read'].append(d.disk_read_bytes)\n",
        "        results['disk_write'].append(d.disk_write_bytes)\n",
        "\n",
        "        if true_labels is not None:\n",
        "            previous_runs_labels.append(labels)\n",
        "\n",
        "            metric_results = {}\n",
        "\n",
        "            # Calculate clustering performance metrics\n",
        "            for metric_name, metric_function in metric_function_map.items():\n",
        "                try:\n",
        "                    value = metric_function(dataset, labels) if metric_name in {'calinski_harabasz', 'davies_bouldin_score', 'silhouette_score', 'cp', 'sp', 'dvi'} else metric_function(true_labels.flatten(), labels.flatten()) if metric_name in {'ari', 'nmi'} else metric_function(dataset, labels.reshape(-1, 1))\n",
        "                    metric_results[metric_name] = value\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error calculating {metric_name}: {e}\")\n",
        "                    metric_results[metric_name] = np.nan\n",
        "\n",
        "            # Add the current run's results to the list of previous runs\n",
        "            previous_runs_results.append(metric_results)\n",
        "\n",
        "    # Stop memory monitoring after the loop\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # Aggregate the results over all runs\n",
        "    for metric_name in metric_function_map.keys():\n",
        "        metric_values = [run_result[metric_name] for run_result in previous_runs_results]\n",
        "        results[f\"{metric_name}_mean\"].append(np.mean(metric_values))\n",
        "        results[f\"{metric_name}_std\"].append(np.std(metric_values))\n",
        "        results[f\"{metric_name}_max\"].append(np.max(metric_values))\n",
        "        results[f\"{metric_name}_min\"].append(np.min(metric_values))\n",
        "\n",
        "    # Calculate min, max, mean, and std of memory usage over all runs\n",
        "    memory_usages = np.array(results['memory_usage'])\n",
        "    memory_stats = {\n",
        "        'memory_min': np.min(memory_usages),\n",
        "        'memory_max': np.max(memory_usages),\n",
        "        'memory_mean': np.mean(memory_usages),\n",
        "        'memory_std': np.std(memory_usages)\n",
        "    }\n",
        "\n",
        "    # Save memory stats\n",
        "    for stat_name, stat_value in memory_stats.items():\n",
        "        results[stat_name].append(stat_value)\n",
        "    results['runtime_mean'] = np.mean(results['runtime'])\n",
        "    results['runtime_std'] = np.std(results['runtime'])\n",
        "    results['runtime_max'] = np.max(results['runtime'])\n",
        "    results['runtime_min'] = np.min(results['runtime'])\n",
        "\n",
        "    return dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#algorithm 1: weighted consensus clustering \n",
        "\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "class ConsensusClustering(BaseEstimator, ClusterMixin):\n",
        "    def __init__(self, n_clusters, model_funcs, weights=None, sampling_rate=0.8, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.model_funcs = model_funcs\n",
        "        self.weights = weights\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        n_samples = X.shape[0]\n",
        "        n_models = len(self.model_funcs)\n",
        "\n",
        "        if self.weights is None:\n",
        "            self.weights = np.ones(n_models)\n",
        "\n",
        "        if len(self.weights) != n_models:\n",
        "            raise ValueError(\"The number of weights must be equal to the number of models.\")\n",
        "\n",
        "        self.weights = self.weights / np.sum(self.weights)\n",
        "\n",
        "        co_association_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "        for i in range(n_models):\n",
        "            model_func = self.model_funcs[i]\n",
        "            model = model_func(self.n_clusters, self.random_state)\n",
        "            sampled_indices = np.random.choice(n_samples, int(n_samples * self.sampling_rate), replace=True)\n",
        "            sampled_data = X[sampled_indices]\n",
        "\n",
        "            if callable(model):\n",
        "                labels = model(sampled_data)\n",
        "            else:\n",
        "                labels = model.fit_predict(sampled_data)\n",
        "\n",
        "            for label in range(self.n_clusters):\n",
        "                cluster_indices = np.where(labels == label)[0]\n",
        "                original_indices = sampled_indices[cluster_indices]\n",
        "                pairs = itertools.combinations(original_indices, 2)\n",
        "                for x, y in pairs:\n",
        "                    co_association_matrix[x, y] += self.weights[i]\n",
        "                    co_association_matrix[y, x] += self.weights[i]\n",
        "\n",
        "        spectral_clustering = SpectralClustering(n_clusters=self.n_clusters, affinity='precomputed')\n",
        "        self.final_labels_ = spectral_clustering.fit_predict(co_association_matrix)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X=None):\n",
        "        return self.final_labels_\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        self.fit(X)\n",
        "        return self.final_labels_\n",
        "        \n",
        "#It is called by the following:\n",
        "consensus_model = ConsensusClustering(n_clusters=3, model_funcs=[KMeans, SpectralClustering])\n",
        "consensus_model.fit(X)\n",
        "labels = consensus_model.predict()"
      ],
      "metadata": {
        "id": "4VlJtTG-M-p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Masters_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    'KMeans': {\n",
        "        'id': 1,\n",
        "        'function': KMeans(n_clusters=3)\n",
        "    },\n",
        "    'DBSCAN': {\n",
        "        'id': 2,\n",
        "        'function': DBSCAN(eps=0.5)\n",
        "    },\n",
        "    'SpectralClustering': {\n",
        "        'id': 3,\n",
        "        'function': SpectralClustering(n_clusters=3)\n",
        "    },\n",
        "    'GaussianMixture': {\n",
        "        'id': 4,\n",
        "        'function': GaussianMixture(n_components=3)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Find all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "\n",
        "# Loop through all CSV files\n",
        "for csv_file in csv_files:\n",
        "    # Use the CSV name as the ID of the dataset\n",
        "    dataset_id = os.path.splitext(os.path.basename(csv_file))[0]\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Extract features and true labels\n",
        "    X = df.drop(columns='label').values\n",
        "    y_true = df['label'].values\n",
        "\n",
        "    # Loop through each model\n",
        "    for model_name, model_info in models.items():\n",
        "        model = model_info['function']\n",
        "\n",
        "        # Evaluate the model on the dataset\n",
        "        results = evaluate_clustering(algorithm=model, dataset=X, true_labels=y_true, n_runs=10)\n",
        "\n",
        "        # Add dataset and model information to the results\n",
        "        results['dataset_id'] = dataset_id\n",
        "        results['model_id'] = model_info['id']\n",
        "        results['model'] = model_name\n",
        "\n",
        "        # Write the results to a new file\n",
        "        results_file_path = os.path.join(folder_path, f'results_{dataset_id}_{model_info[\"id\"]}.json')\n",
        "        with open(results_file_path, 'w') as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "        # Append the results to the final_evaluation_results file\n",
        "        final_results_file_path = os.path.join(folder_path, 'final_evaluation_results.json')\n",
        "        if os.path.exists(final_results_file_path):\n",
        "            with open(final_results_file_path, 'r') as f:\n",
        "                final_results = json.load(f)\n",
        "        else:\n",
        "            final_results = []\n",
        "\n",
        "        final_results.append(results)\n",
        "\n",
        "        with open(final_results_file_path, 'w') as f:\n",
        "            json.dump(final_results, f)\n"
      ],
      "metadata": {
        "id": "c-_TIbK5J-yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}