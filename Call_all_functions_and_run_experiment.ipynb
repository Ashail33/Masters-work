{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJJRQx+dHW0rvzg4reRW0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashail33/Masters-work/blob/master/Call_all_functions_and_run_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"my-app-name\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml.clustering import KMeans, KMeansModel\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import cdist\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import tracemalloc\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import json\n",
        "\n",
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "epW9jv8BSYCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2LB8VkjJ-JU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#evaluation code\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.monotonic()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.interval = time.monotonic() - self.start_time\n",
        "\n",
        "\n",
        "class MemoryMonitor:\n",
        "    def __enter__(self):\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.memory_info = self.process.memory_info()\n",
        "\n",
        "\n",
        "class DiskMonitor:\n",
        "    def __enter__(self):\n",
        "        self.disk_read_start = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_start = psutil.disk_io_counters().write_bytes\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.disk_read_end = psutil.disk_io_counters().read_bytes\n",
        "        self.disk_write_end = psutil.disk_io_counters().write_bytes\n",
        "        self.disk_read_bytes = self.disk_read_end - self.disk_read_start\n",
        "        self.disk_write_bytes = self.disk_write_end - self.disk_write_start\n",
        "\n",
        "\n",
        "def compactness(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([np.mean(X[labels == i], axis=0) for i in unique_labels])\n",
        "    distances = cdist(X, centroids)\n",
        "    return np.sum(np.min(distances, axis=1))\n",
        "\n",
        "\n",
        "def separation(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    centroids = np.array([np.mean(X[labels == i], axis=0) for i in unique_labels])\n",
        "    return np.sum(cdist(centroids, centroids))\n",
        "\n",
        "\n",
        "\n",
        "def batched_silhouette_score(X, labels, batch_size=500):\n",
        "    labels = labels.flatten()\n",
        "    n_samples = len(X)\n",
        "    n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    silhouette_sum = 0\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        pairwise_distances = sklearn.metrics.pairwise_distances(X[start:end], X)\n",
        "\n",
        "        a = np.array([np.mean(pairwise_distances[i, labels[start:end] == labels[start + i]]) for i in range(end-start)])\n",
        "        b = np.array([np.min([np.mean(pairwise_distances[i, labels[start:end] == cur_label]) \n",
        "                              for cur_label in set(labels) - {labels[start + i]}]) \n",
        "                      for i in range(end-start)])\n",
        "\n",
        "        silhouette_sum += np.sum((b - a) / np.maximum(a, b))\n",
        "\n",
        "    return silhouette_sum / n_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dunn_index(X, labels):\n",
        "    labels = labels.flatten()\n",
        "    unique_labels = np.unique(labels)\n",
        "    clusters = [X[labels == i] for i in unique_labels]\n",
        "    max_intra_cluster_distance = np.max([np.max(cdist(c1, c1)) for c1 in clusters])\n",
        "\n",
        "    min_inter_cluster_distance = np.inf\n",
        "    for i, c1 in enumerate(clusters):\n",
        "        for j, c2 in enumerate(clusters):\n",
        "            if i < j:\n",
        "                min_distance = np.min(cdist(c1, c2))\n",
        "                if min_distance < min_inter_cluster_distance:\n",
        "                    min_inter_cluster_distance = min_distance\n",
        "\n",
        "    return min_inter_cluster_distance / max_intra_cluster_distance\n",
        "\n",
        "from scipy.spatial.distance import pdist, cdist\n",
        "\n",
        "def batched_dunn_index(X, labels, batch_size=500):\n",
        "    n_samples = len(X)\n",
        "    n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "    max_intra_cluster_distance = -np.inf\n",
        "    min_inter_cluster_distance = np.inf\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, n_samples)\n",
        "\n",
        "        batch = X[start:end]\n",
        "        batch_labels = labels[start:end]\n",
        "\n",
        "        unique_labels = np.unique(batch_labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            cluster = batch[batch_labels == label]\n",
        "            intra_cluster_distance = np.max(pdist(cluster))\n",
        "\n",
        "            if intra_cluster_distance > max_intra_cluster_distance:\n",
        "                max_intra_cluster_distance = intra_cluster_distance\n",
        "\n",
        "        for i, c1 in enumerate(unique_labels):\n",
        "            for j, c2 in enumerate(unique_labels):\n",
        "                if i < j:\n",
        "                    inter_cluster_distance = np.min(cdist(X[labels == c1], X[labels == c2]))\n",
        "\n",
        "                    if inter_cluster_distance < min_inter_cluster_distance:\n",
        "                        min_inter_cluster_distance = inter_cluster_distance\n",
        "\n",
        "    return min_inter_cluster_distance / max_intra_cluster_distance\n",
        "\n",
        "\n",
        "# code to evaluate the clustering algorithms\n",
        "def evaluate_clustering(algorithm, dataset, true_labels=None, n_runs=10):\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    metric_function_map = {\n",
        "        'cp': compactness,\n",
        "        'sp': separation,\n",
        "        'db': davies_bouldin_score,\n",
        "        'silhouette': batched_silhouette_score,\n",
        "        'calinski_harabasz': calinski_harabasz_score,\n",
        "        'ari': adjusted_rand_score,\n",
        "        'nmi': normalized_mutual_info_score,\n",
        "        'dvi': batched_dunn_index,\n",
        "    }\n",
        "\n",
        "    previous_runs_labels = []\n",
        "    previous_runs_results = []\n",
        "\n",
        "    # Start memory monitoring before the loop\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        with Timer() as t, DiskMonitor() as d:\n",
        "            labels = algorithm.fit_predict(dataset)\n",
        "\n",
        "        # Memory usage tracking\n",
        "        current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "        results['memory_usage'].append(current_memory)\n",
        "        results['memory_peak'].append(peak_memory)\n",
        "\n",
        "        results['runtime'].append(t.interval)\n",
        "        results['disk_read'].append(d.disk_read_bytes)\n",
        "        results['disk_write'].append(d.disk_write_bytes)\n",
        "\n",
        "        if true_labels is not None:\n",
        "            previous_runs_labels.append(labels)\n",
        "\n",
        "            metric_results = {}\n",
        "\n",
        "            # Calculate clustering performance metrics\n",
        "            for metric_name, metric_function in metric_function_map.items():\n",
        "                try:\n",
        "                    value = metric_function(dataset, labels) if metric_name in {'calinski_harabasz', 'davies_bouldin_score', 'silhouette_score', 'cp', 'sp', 'dvi'} else metric_function(true_labels.flatten(), labels.flatten()) if metric_name in {'ari', 'nmi'} else metric_function(dataset, labels.reshape(-1, 1))\n",
        "                    metric_results[metric_name] = value\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error calculating {metric_name}: {e}\")\n",
        "                    metric_results[metric_name] = np.nan\n",
        "\n",
        "            # Add the current run's results to the list of previous runs\n",
        "            previous_runs_results.append(metric_results)\n",
        "\n",
        "    # Stop memory monitoring after the loop\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # Aggregate the results over all runs\n",
        "    for metric_name in metric_function_map.keys():\n",
        "        metric_values = [run_result[metric_name] for run_result in previous_runs_results]\n",
        "        results[f\"{metric_name}_mean\"].append(np.mean(metric_values))\n",
        "        results[f\"{metric_name}_std\"].append(np.std(metric_values))\n",
        "        results[f\"{metric_name}_max\"].append(np.max(metric_values))\n",
        "        results[f\"{metric_name}_min\"].append(np.min(metric_values))\n",
        "\n",
        "    # Calculate min, max, mean, and std of memory usage over all runs\n",
        "    memory_usages = np.array(results['memory_usage'])\n",
        "    memory_stats = {\n",
        "        'memory_min': np.min(memory_usages),\n",
        "        'memory_max': np.max(memory_usages),\n",
        "        'memory_mean': np.mean(memory_usages),\n",
        "        'memory_std': np.std(memory_usages)\n",
        "    }\n",
        "\n",
        "    # Save memory stats\n",
        "    for stat_name, stat_value in memory_stats.items():\n",
        "        results[stat_name].append(stat_value)\n",
        "    results['runtime_mean'] = np.mean(results['runtime'])\n",
        "    results['runtime_std'] = np.std(results['runtime'])\n",
        "    results['runtime_max'] = np.max(results['runtime'])\n",
        "    results['runtime_min'] = np.min(results['runtime'])\n",
        "\n",
        "    return dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use this function for weighted consensus method, it has it's issues like the fit to complex shapes irrespective of the underlying model, the spectral clustering of the affinity matrix makes this tricky. This is definitely a shortfall of the method\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "import numpy as np\n",
        "\n",
        "class ConsensusClustering:\n",
        "    def __init__(self, n_clusters, model_funcs, weights=None, sampling_rate=0.8, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.model_funcs = model_funcs\n",
        "        self.weights = weights\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.random_state = random_state\n",
        "        self.co_association_matrix = None\n",
        "\n",
        "    def fit_predict(self, data):\n",
        "        n_samples = data.shape[0]\n",
        "        n_models = len(self.model_funcs)\n",
        "\n",
        "        if self.weights is None:\n",
        "            self.weights = np.ones(n_models)\n",
        "\n",
        "        if len(self.weights) != n_models:\n",
        "            raise ValueError(\"The number of weights must be equal to the number of models.\")\n",
        "\n",
        "        self.weights = self.weights / np.sum(self.weights)\n",
        "\n",
        "        self.co_association_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "        for i in range(n_models):\n",
        "            model_func = self.model_funcs[i]\n",
        "            model = model_func(self.n_clusters)\n",
        "            sampled_indices = np.random.choice(n_samples, int(n_samples * self.sampling_rate), replace=True)\n",
        "            sampled_data = data[sampled_indices]\n",
        "            \n",
        "            if callable(model):\n",
        "                labels = model(sampled_data)\n",
        "            else:\n",
        "                labels = model.fit_predict(sampled_data)\n",
        "\n",
        "            for label in range(self.n_clusters):\n",
        "                cluster_indices = np.where(labels == label)[0]\n",
        "                original_indices = sampled_indices[cluster_indices]\n",
        "                pairs = itertools.combinations(original_indices, 2)\n",
        "                for x, y in pairs:\n",
        "                    self.co_association_matrix[x, y] += self.weights[i]\n",
        "                    self.co_association_matrix[y, x] += self.weights[i]\n",
        "\n",
        "        spectral_clustering = SpectralClustering(n_clusters=self.n_clusters, affinity='precomputed')\n",
        "        final_labels = spectral_clustering.fit_predict(self.co_association_matrix)\n",
        "\n",
        "        return final_labels\n",
        "\n",
        "\n",
        "# Model functions for k-means and Agglomerative clustering\n",
        "def kmeans_model_func(n_clusters, random_state):\n",
        "    return KMeans(n_clusters=n_clusters, random_state=random_state)\n",
        "\n",
        "def agglomerative_model_func(n_clusters, random_state):\n",
        "    return AgglomerativeClustering(n_clusters=n_clusters)\n",
        "\n",
        "def dbscan_model_func(n_clusters):\n",
        "    return DBSCAN(eps=0.15, min_samples=10)\n",
        "\n",
        "def optics_model_func(n_clusters, random_state):\n",
        "    return OPTICS(min_samples=1, xi=0.5, cluster_method='xi', n_jobs=-1)\n",
        "\n",
        "def optics_DB_Scan_model_func(n_clusters, random_state):\n",
        "    def optics_instance(data):\n",
        "        optics = OPTICS(min_samples=1, n_jobs=-1)\n",
        "        optics.fit(data)\n",
        "        eps = np.partition(optics.reachability_[optics.ordering_], n_clusters - 1)[n_clusters - 1]\n",
        "        labels = cluster_optics_dbscan(reachability=optics.reachability_,\n",
        "                                       core_distances=optics.core_distances_,\n",
        "                                       ordering=optics.ordering_,\n",
        "                                       eps=eps)\n",
        "        return labels\n",
        "\n",
        "    return optics_instance   \n",
        "\n",
        "# Define your models in a list\n",
        "model_funcs = [KMeans]*5\n",
        "\n",
        "# Define the weights for the models\n",
        "weights = [0.5, 0.2,0.003,0.4,3]\n",
        "\n",
        "# # Initialize ConsensusClustering\n",
        "# consensus_clustering = ConsensusClustering(n_clusters=2, model_funcs=model_funcs, weights=weights, sampling_rate=0.8)\n",
        "\n",
        "# # Load your data\n",
        "# data, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# # Apply consensus clustering\n",
        "# labels = consensus_clustering.fit_predict(data)\n",
        "\n",
        "# # `labels` now contains the cluster assignments for each sample in `data`.\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plot the data points with colors corresponding to the assigned cluster\n",
        "# plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
        "\n",
        "# # Title and axis labels\n",
        "# plt.title('Consensus Clustering Results')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "Yp_NYTGSwPHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, load_digits, make_blobs\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "# Set up SparkSession\n",
        "spark = SparkSession.builder.appName(\"MapReduceKMeans\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "\n",
        "# def mapreduce_kmeans(data, n_clusters, batch_size, max_iterations):\n",
        "#     \"\"\"\n",
        "#     MapReduce KMeans implementation using PySpark.\n",
        "#     \"\"\"\n",
        "#     spark = SparkSession.builder.getOrCreate()\n",
        "#     sc = spark.sparkContext\n",
        "#     sc.setLogLevel(\"ERROR\")\n",
        "#     # Convert numpy array to PySpark DataFrame\n",
        "#     df = sc.parallelize(data.tolist()).map(lambda x: (Vectors.dense(x), )).toDF([\"features\"])\n",
        "#     # Initialize centroids using KMeans++ initialization\n",
        "#     initial_centroids = PKMeans().fit(df.select(\"features\")).clusterCenters()\n",
        "#     global_centroids = initial_centroids\n",
        "#     for i in range(0, data.shape[0], batch_size):\n",
        "#         # Split data into batches\n",
        "#         local_data = data[i:i+batch_size]\n",
        "#         local_df = sc.parallelize(local_data.tolist()).map(lambda x: (Vectors.dense(x), )).toDF([\"features\"])\n",
        "#         # Train local KMeans model\n",
        "#         local_kmeans = PKMeans(k=n_clusters, initMode=\"k-means||\", maxIter=max_iterations, seed=42)\n",
        "#         local_model = local_kmeans.fit(local_df.select(\"features\"))\n",
        "#         local_centroids = local_model.clusterCenters()\n",
        "#         # Update global centroids\n",
        "#         global_centroids = np.vstack([global_centroids, local_centroids])\n",
        "#         global_centroids = SKKMeans(n_clusters=n_clusters, random_state=42).fit(global_centroids).cluster_centers_\n",
        "#     # Final KMeans model with global centroids\n",
        "#     kmeans = PKMeans(k=n_clusters, initMode=\"k-means||\", maxIter=max_iterations, seed=42)\n",
        "#     final_model = kmeans.fit(df.select(\"features\"))\n",
        "#     evaluator = ClusteringEvaluator()\n",
        "#     global_cost = evaluator.evaluate(final_model.transform(df))\n",
        "#     global_centroids = final_model.clusterCenters()\n",
        "#     return KMeans(n_clusters=n_clusters, init=global_centroids, n_init=1, random_state=42)\n",
        "# # MapReduce KMeans\n",
        "# time_mbkmeans = []\n",
        "# inertia_mbkmeans = []\n",
        "# # Load datasets\n",
        "# iris_data = load_iris().data\n",
        "# digits_data = load_digits().data\n",
        "# blobs_data = make_blobs(n_samples=10000, centers=3, n_features=2, random_state=42)[0]\n",
        "\n",
        "# # Define parameters for KMeans and MapReduce KMeans\n",
        "# n_clusters = 3\n",
        "# batch_size = 1000\n",
        "# max_iterations = 100\n",
        "\n",
        "# for data in [iris_data, digits_data, blobs_data]:\n",
        "#     mbkmeans= mapreduce_kmeans(data, n_clusters, batch_size, max_iterations)\n",
        "#     # mbkmeans = KMeans(n_clusters=n_clusters, init=global_centroids, n_init=1, random_state=42)\n",
        "#     time_taken = %timeit -o -r 1 -n 1 mbkmeans.fit(data)\n",
        "#     time_mbkmeans.append(time_taken.best)\n",
        "#     inertia_mbkmeans.append(mbkmeans.inertia_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\n",
        "# # Regular KMeans\n",
        "# time_kmeans = []\n",
        "# inertia_kmeans = []\n",
        "# for data in [iris_data, digits_data, blobs_data]:\n",
        "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "#     result = %timeit -o -r 1 -n 1 kmeans.fit(data)\n",
        "#     time_kmeans.append(result.best)\n",
        "#     inertia_kmeans.append(kmeans.inertia_)\n",
        "# # Visualize results\n",
        "# datasets = [\"Iris\", \"Digits\", \"Blobs\"]\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# ax1.bar(datasets, time_kmeans, label=\"KMeans\")\n",
        "# ax1.bar(datasets, time_mbkmeans, bottom=time_kmeans, label=\"MapReduceKMeans\")\n",
        "# ax1.set_title(\"Runtime Comparison\")\n",
        "# ax1.set_ylabel(\"Time (s)\")\n",
        "# ax1.legend()\n",
        "\n",
        "# ax2.bar(datasets, inertia_kmeans, label=\"KMeans\")\n",
        "# ax2.bar(datasets, inertia_mbkmeans, bottom=inertia_kmeans, label=\"MapReduceKMeans\")\n",
        "# ax2.set_title(\"Inertia Comparison\")\n",
        "# ax2.set_ylabel(\"Inertia\")\n",
        "# ax2.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "-k0W8hYsNZNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, load_digits, make_blobs\n",
        "from sklearn.cluster import KMeans as SKKMeans\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "# Set up SparkSession\n",
        "spark = SparkSession.builder.appName(\"MapReduceKMeans\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "from pyspark.ml.clustering import KMeans as PKMeans\n",
        "\n",
        "class MapReduceKMeans:\n",
        "    def __init__(self, n_clusters, batch_size, max_iterations):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "    def fit_predict(self, data):\n",
        "        \"\"\"\n",
        "        MapReduce KMeans implementation using PySpark.\n",
        "        \"\"\"\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        sc = spark.sparkContext\n",
        "        sc.setLogLevel(\"ERROR\")\n",
        "        # Convert numpy array to PySpark DataFrame\n",
        "        df = sc.parallelize(data.tolist()).map(lambda x: (Vectors.dense(x), )).toDF([\"features\"])\n",
        "        # Initialize centroids using KMeans++ initialization\n",
        "        initial_centroids = PKMeans().fit(df.select(\"features\")).clusterCenters()\n",
        "        global_centroids = initial_centroids\n",
        "        for i in range(0, data.shape[0], self.batch_size):\n",
        "            # Split data into batches\n",
        "            local_data = data[i:i+self.batch_size]\n",
        "            local_df = sc.parallelize(local_data.tolist()).map(lambda x: (Vectors.dense(x), )).toDF([\"features\"])\n",
        "            # Train local KMeans model\n",
        "            local_kmeans = PKMeans(k=self.n_clusters, initMode=\"k-means||\", maxIter=self.max_iterations, seed=42)\n",
        "            local_model = local_kmeans.fit(local_df.select(\"features\"))\n",
        "            local_centroids = local_model.clusterCenters()\n",
        "            # Update global centroids\n",
        "            global_centroids = np.vstack([global_centroids, local_centroids])\n",
        "            global_centroids = SKKMeans(n_clusters=self.n_clusters, random_state=42).fit(global_centroids).cluster_centers_\n",
        "        # Final KMeans model with global centroids\n",
        "        kmeans = PKMeans(k=self.n_clusters, initMode=\"k-means||\", maxIter=self.max_iterations, seed=42)\n",
        "        final_model = kmeans.fit(df.select(\"features\"))\n",
        "        evaluator = ClusteringEvaluator()\n",
        "        global_cost = evaluator.evaluate(final_model.transform(df))\n",
        "        global_centroids = final_model.clusterCenters()\n",
        "        return KMeans(n_clusters=self.n_clusters, init=global_centroids, n_init=1, random_state=42).fit_predict(data)\n",
        "\n",
        "\n",
        "# # MapReduce KMeans\n",
        "# time_mbkmeans = []\n",
        "# inertia_mbkmeans = []\n",
        "# # Load datasets\n",
        "# iris_data = load_iris().data\n",
        "# digits_data = load_digits().data\n",
        "# blobs_data = make_blobs(n_samples=10000, centers=3, n_features=2, random_state=42)[0]\n",
        "\n",
        "# # Define parameters for KMeans and MapReduce KMeans\n",
        "# n_clusters = 3\n",
        "# batch_size = 1000\n",
        "# max_iterations = 100\n",
        "\n",
        "# for data in [iris_data, digits_data, blobs_data]:\n",
        "#     mbkmeans= mapreduce_kmeans(data, n_clusters, batch_size, max_iterations)\n",
        "#     # mbkmeans = KMeans(n_clusters=n_clusters, init=global_centroids, n_init=1, random_state=42)\n",
        "#     time_taken = %timeit -o -r 1 -n 1 mbkmeans.fit(data)\n",
        "#     time_mbkmeans.append(time_taken.best)\n",
        "#     inertia_mbkmeans.append(mbkmeans.inertia_)\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\n",
        "# # Regular KMeans\n",
        "# time_kmeans = []\n",
        "# inertia_kmeans = []\n",
        "# for data in [iris_data, digits_data, blobs_data]:\n",
        "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "#     result = %timeit -o -r 1 -n 1 kmeans.fit(data)\n",
        "#     time_kmeans.append(result.best)\n",
        "#     inertia_kmeans.append(kmeans.inertia_)\n",
        "# # Visualize results\n",
        "# datasets = [\"Iris\", \"Digits\", \"Blobs\"]\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# ax1.bar(datasets, time_kmeans, label=\"KMeans\")\n",
        "# ax1.bar(datasets, time_mbkmeans, bottom=time_kmeans, label=\"MapReduceKMeans\")\n",
        "# ax1.set_title(\"Runtime Comparison\")\n",
        "# ax1.set_ylabel(\"Time (s)\")\n",
        "# ax1.legend()\n",
        "\n",
        "# ax2.bar(datasets, inertia_kmeans, label=\"KMeans\")\n",
        "# ax2.bar(datasets, inertia_mbkmeans, bottom=inertia_kmeans, label=\"MapReduceKMeans\")\n",
        "# ax2.set_title(\"Inertia Comparison\")\n",
        "# ax2.set_ylabel(\"Inertia\")\n",
        "# ax2.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "1rpEEPsbzNNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Usage:\n",
        "# from sklearn.datasets import make_moons\n",
        "\n",
        "# # Load the moons dataset\n",
        "# moons_data, moons_labels = make_moons(n_samples=500, noise=0.001, random_state=42)\n",
        "\n",
        "# # Apply SSClustering\n",
        "# ss_clustering = mapreduce_kmeans(data, 2, batch_size, max_iterations)\n",
        "# cluster_assignments = ss_clustering.fit_predict(moons_data)\n",
        "\n",
        "\n",
        "# # Plot the results\n",
        "# plt.scatter(moons_data[:, 0], moons_data[:, 1], c=cluster_assignments, cmap='viridis')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "tzl5rRl50AeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "# from sklearn.datasets import make_blobs\n",
        "\n",
        "# # Generate some random data\n",
        "# X, y = make_blobs(n_samples=100000, centers=10, n_features=100, random_state=42)\n",
        "\n",
        "# # Create and fit the MiniBatchKMeans model\n",
        "# batch_size = 1000\n",
        "# n_clusters = 10\n",
        "# mbkmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, random_state=42)\n",
        "# mbkmeans.fit(X)"
      ],
      "metadata": {
        "id": "sdcLqztQgUEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import scipy.sparse.linalg as sp_linalg\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class SSClustering(BaseEstimator, ClusterMixin):\n",
        "    def __init__(self, n_clusters=2, alpha=0.09, l1_ratio=0.2, k=10):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.l1_ratio = l1_ratio\n",
        "        self.k = k\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_columns(X):\n",
        "        norms = np.linalg.norm(X, axis=0)\n",
        "        return X / norms[np.newaxis, :]\n",
        "\n",
        "    def compute_sparse_representation(self, X):\n",
        "        n_samples = X.shape[1]\n",
        "        A = np.zeros((n_samples, n_samples))\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            x_i = X[:, i]\n",
        "            X_rest = np.delete(X, i, axis=1)\n",
        "            elastic_net = ElasticNet(alpha=self.alpha, l1_ratio=self.l1_ratio, fit_intercept=False)\n",
        "            elastic_net.fit(X_rest, x_i)\n",
        "            coeffs = elastic_net.coef_\n",
        "            A[np.arange(n_samples) != i, i] = coeffs\n",
        "\n",
        "        return A\n",
        "\n",
        "    def spectral_clustering(self, A):\n",
        "        Laplacian = np.diag(np.sum(np.abs(A), axis=0)) - A\n",
        "        eigvals, eigvecs = np.linalg.eig(Laplacian)\n",
        "        idx = eigvals.argsort()[:self.n_clusters]\n",
        "        U = np.real(eigvecs[:, idx])\n",
        "\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=0)\n",
        "        return kmeans.fit_predict(U)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        X = self.normalize_columns(X.T)\n",
        "        A = self.compute_sparse_representation(X)\n",
        "        A = np.abs(A) + np.abs(A.T)\n",
        "\n",
        "        if self.k is not None:\n",
        "            for i in range(A.shape[0]):\n",
        "                idx = np.argsort(A[i, :])[:-self.k - 1:-1]\n",
        "                A[i, np.setdiff1d(np.arange(A.shape[1]), idx)] = 0\n",
        "\n",
        "        return self.spectral_clustering(A)\n",
        "\n",
        "\n",
        "# # Usage:\n",
        "# from sklearn.datasets import make_moons\n",
        "\n",
        "# # Load the moons dataset\n",
        "# moons_data, moons_labels = make_moons(n_samples=500, noise=0.001, random_state=42)\n",
        "\n",
        "# # Apply SSClustering\n",
        "# ss_clustering = SSClustering(n_clusters=2, alpha=0.09, l1_ratio=0.2, k=10)\n",
        "# cluster_assignments = ss_clustering.fit_predict(moons_data)\n",
        "\n",
        "\n",
        "# # Plot the results\n",
        "# plt.scatter(moons_data[:, 0], moons_data[:, 1], c=cluster_assignments, cmap='viridis')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "FytxBaj8hZUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from sklearn.base import BaseEstimator, ClusterMixin\n",
        "# from sklearn.linear_model import ElasticNet\n",
        "# import scipy.sparse.linalg as sp_linalg\n",
        "# from sklearn.cluster import KMeans\n",
        "# import scipy.sparse\n",
        "\n",
        "# class SSClustering_powermethod(BaseEstimator, ClusterMixin):\n",
        "#     def __init__(self, n_clusters=2, alpha=0.09, l1_ratio=0.2, k=10, spectral_model=KMeans(n_clusters=n_clusters)):\n",
        "#         self.n_clusters = n_clusters\n",
        "#         self.alpha = alpha\n",
        "#         self.l1_ratio = l1_ratio\n",
        "#         self.k = k\n",
        "#         self.spectral_model= spectral_model\n",
        "\n",
        "#     @staticmethod\n",
        "#     def normalize_columns(X):\n",
        "#         norms = np.linalg.norm(X, axis=0)\n",
        "#         return X / norms[np.newaxis, :]\n",
        "\n",
        "#     def compute_sparse_representation(self, X):\n",
        "#         n_samples = X.shape[1]\n",
        "#         A = np.zeros((n_samples, n_samples))\n",
        "        \n",
        "#         for i in range(n_samples):\n",
        "#             x_i = X[:, i]\n",
        "#             X_rest = np.delete(X, i, axis=1)\n",
        "#             elastic_net = ElasticNet(alpha=self.alpha, l1_ratio=self.l1_ratio, fit_intercept=False)\n",
        "#             elastic_net.fit(X_rest, x_i)\n",
        "#             coeffs = elastic_net.coef_\n",
        "#             A[np.arange(n_samples) != i, i] = coeffs\n",
        "\n",
        "#         return A\n",
        "\n",
        "#     def spectral_clustering(self, A):\n",
        "#         Laplacian = np.diag(np.sum(np.abs(A), axis=0)) - A\n",
        "#         # Convert Laplacian to sparse matrix for efficiency\n",
        "#         Laplacian_sparse = scipy.sparse.csr_matrix(Laplacian)\n",
        "        \n",
        "#         # Compute the first few eigenvectors of the Laplacian\n",
        "#         eigvals, eigvecs = sp_linalg.eigs(Laplacian_sparse, k=self.n_clusters, which='SM')\n",
        "\n",
        "#         U = np.real(eigvecs)\n",
        "\n",
        "#         kmeans = self.spectral_model\n",
        "\n",
        "#         if callable(kmeans):\n",
        "#             labels = kmeans(self.n_clusters).fit_predict(U)\n",
        "#         else:\n",
        "#             labels = kmeans.fit_predict(U)\n",
        "#         return labels\n",
        "\n",
        "#     def fit_predict(self, X, y=None):\n",
        "#         X = self.normalize_columns(X.T)\n",
        "#         A = self.compute_sparse_representation(X)\n",
        "#         A = np.abs(A) + np.abs(A.T)\n",
        "\n",
        "#         if self.k is not None:\n",
        "#             for i in range(A.shape[0]):\n",
        "#                 idx = np.argsort(A[i, :])[:-self.k - 1:-1]\n",
        "#                 A[i, np.setdiff1d(np.arange(A.shape[1]), idx)] = 0\n",
        "\n",
        "#         return self.spectral_clustering(A)\n",
        "\n",
        "\n",
        "# # # Usage:\n",
        "# # from sklearn.datasets import make_moons\n",
        "\n",
        "# # # Load the moons dataset\n",
        "# # moons_data, moons_labels = make_moons(n_samples=500, noise=0.001, random_state=42)\n",
        "\n",
        "# # def dbscan_model_func(n_clusters):\n",
        "# #     return DBSCAN(eps=0.15, min_samples=10)\n",
        "# # # Apply SSClustering\n",
        "# # ss_clustering = SSClustering_powermethod(n_clusters=10, alpha=0.7, l1_ratio=0.002, k=1000,)\n",
        "# # cluster_assignments = ss_clustering.fit_predict(moons_data)\n",
        "\n",
        "\n",
        "# # # Plot the results\n",
        "# # plt.scatter(moons_data[:, 0], moons_data[:, 1], c=cluster_assignments, cmap='viridis')\n",
        "# # plt.show()\n"
      ],
      "metadata": {
        "id": "bOwXbr_WjSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "# Set the path to the 'Masters_data' folder in your Google Drive\n",
        "base_path = '/content/gdrive/MyDrive/'\n",
        "folder_name = 'Master_test_data'\n",
        "folder_path = os.path.join(base_path, folder_name)\n",
        "n_clusters = 3\n",
        "batch_size=10\n",
        "sampling_rate=0.9\n",
        "\\\n",
        "# Define the models\n",
        "models = {\n",
        "    'KMeans': {\n",
        "        'id': 1,\n",
        "        'function': KMeans(n_clusters=n_clusters)\n",
        "    },\n",
        "    'S5C': {\n",
        "        'id': 2,\n",
        "        'function': SSClustering(n_clusters=n_clusters, alpha=0.09, l1_ratio=0.2, k=10)\n",
        "    },\n",
        "    'Minibatch_clustering': {\n",
        "        'id': 3,\n",
        "        'function': MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, random_state=42)\n",
        "    },\n",
        "    'Parallel K-means': {\n",
        "        'id': 4,\n",
        "        'function': MapReduceKMeans(n_clusters=n_clusters, batch_size=10, max_iterations=100)\n",
        "    },\n",
        "    'Weighted Consensus Clustering': {\n",
        "        'id': 5,\n",
        "        'function': ConsensusClustering(n_clusters, model_funcs, weights, sampling_rate)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Find all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "print(csv_files)\n",
        "class NumPyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NumPyEncoder, self).default(obj)\n",
        "\n",
        "\n",
        "\n",
        "# Loop through all CSV files\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)\n",
        "    # Use the CSV name as the ID of the dataset\n",
        "    dataset_id = os.path.splitext(os.path.basename(csv_file))[0]\n",
        "\n",
        "    # Load the dataset without headers\n",
        "    df = pd.read_csv(csv_file, header=None)\n",
        "\n",
        "    # Extract features and true labels\n",
        "    X = df.iloc[:, :-1].values  # all columns except the last\n",
        "    y_true = df.iloc[:, -1].values  # last column\n",
        "\n",
        "    # Loop through each model\n",
        "    for model_name, model_info in models.items():\n",
        "        print(model_name)\n",
        "        model = model_info['function']\n",
        "\n",
        "        # Evaluate the model on the dataset\n",
        "        results = evaluate_clustering(algorithm=model, dataset=X, true_labels=y_true, n_runs=1)\n",
        "\n",
        "        # Add dataset and model information to the results\n",
        "        results['dataset_id'] = dataset_id\n",
        "        results['model_id'] = model_info['id']\n",
        "        results['model'] = model_name\n",
        "\n",
        "        # Write the results to a new file\n",
        "        results_file_path = os.path.join(folder_path, f'results.json')\n",
        "        with open(results_file_path, 'a') as f:\n",
        "            # And then when dumping the JSON:\n",
        "            json.dump(results, f, cls=NumPyEncoder)\n",
        "\n",
        "        # Append the results to the final_evaluation_results file\n",
        "        final_results_file_path = os.path.join(folder_path, 'final_evaluation_results.json')\n",
        "        if os.path.exists(final_results_file_path):\n",
        "            with open(final_results_file_path, 'r') as f:\n",
        "                final_results = json.load(f)\n",
        "        else:\n",
        "            final_results = []\n",
        "\n",
        "        final_results.append(results)\n",
        "\n",
        "        with open(final_results_file_path, 'w') as f:\n",
        "            json.dump(final_results, f)\n"
      ],
      "metadata": {
        "id": "c-_TIbK5J-yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}